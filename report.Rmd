---
title: "Predicting measurement type of UCI Ionosphere radar data"
author: "cgoldner"
date: "`r format(Sys.Date())`"
header-includes:
    - \usepackage{amsmath}
    - \usepackage{hyperref}
    - \usepackage{dcolumn}
    - \usepackage{here}
output:
#  github_document
   pdf_document:
     latex_engine: pdflatex
     number_sections: true
---
```{r setup, include=FALSE, echo = FALSE, results = 'hide', warning = FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.width = 10,
                      fig.height = 5,
                      cache = TRUE) # use cache = FALSE for final version

# load knitr libraries
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
library(knitr)
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
library(kableExtra)

# load other libraries
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
library(tidyverse)
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
library(caret)
if(!require(corrplot)) install.packages("corrplot", repos = "http://cran.us.r-project.org")
library(corrplot)
if(!require(naniar)) install.packages("naniar", repos = "http://cran.us.r-project.org")
library(naniar)
if(!require(grid)) install.packages("grid", repos = "http://cran.us.r-project.org")
library(grid)
if(!require(RColorBrewer)) install.packages("RColorBrewer", repos = "http://cran.us.r-project.org")
library(RColorBrewer)
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org")
library(gridExtra)
if(!require(matrixStats)) install.packages("matrixStats", repos = "http://cran.us.r-project.org")
library(matrixStats)
```

```{r download_data, echo = FALSE, message=FALSE, results='hide', warning=FALSE}
# Download data, wrangle it and save it -----------------------------------

## The original source of the data is
## https://archive.ics.uci.edu/ml/datasets/Ionosphere
## url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data"


# download data
url <- "https://raw.githubusercontent.com/cgoldner/radar-ionosphere/main/data/ionosphere.data"
dest_file <- "data/ionosphere.data"
if(!exists("ionosphere")) download.file(url, destfile = dest_file)

# read data
ionosphere <- read.table("data/ionosphere.data", sep = ",")

# inspect data
str(ionosphere)
names(ionosphere)
nrow(ionosphere)

# rename columns
ionosphere <- setNames(ionosphere, c(paste0("x_",1:34), "y"))

# convert all predictor variables to numeric and convert outcome to factor
ionosphere <- ionosphere %>% mutate(x_1 = as.numeric(x_1),
                                    x_2 = as.numeric(x_2),
                                    y = factor(y, levels = c("g", "b")))

# Create train and validation set -----------------------------------------

# split off validation set for later use
# split is train = 0.8 and validation = 0.2
set.seed(1, sample.kind="Rounding") # set seed to make splitting reproducible
val_index <- createDataPartition(ionosphere$y, times = 1, p = 0.2, list = FALSE)
validation <- ionosphere %>% slice(val_index)
training <- ionosphere %>% slice(-val_index)
```

\tableofcontents
\newpage

# Overview
<!-- an introduction/overview/executive summary section that describes the dataset and variables, and summarizes the goal of the project and key steps that were performed -->

## UCI Ionosphere radar dataset
The Ionosphere radar data was collected by a system in Goose Bay, Labrador. The system collected data via high-frequency antennas. The "good" radar returns are those showing evidence of some type of structure in the ionosphere. "Bad" returns are those that do not; their signals pass through the ionosphere. Received signals were processed such that the outputs of each measurement are $17$ \textit{pulses}. Each pulse is described by $2$ \textit{attributes}, corresponding to the real and imaginary part of a complex number. The data is freely available from the UCI Machine Learning Repository
\begin{center}
https://archive.ics.uci.edu/ml/datasets/Ionosphere.
\end{center}


## Goal of the project
The goal of this project is to build a data-driven classification model which predicts whether a measurement is "good" or "bad" given the measured data, such that the model's performance is as good as possible.

### Measure of performance
The model's performance is evaluated by measuring the \textit{accuracy} of the model on a \textit{validation set}. Accuracy is defined as
\begin{align*}
\textrm{accuracy} = \frac{\textrm{number of right predictions on the validation set}}{\textrm{number of elements in the validation set}}.
\end{align*}

## Key steps
The most important steps performed in order to build a model for classifying "good" and "bad" measurements are:
\begin{itemize}
\item[(1)]
Download the data, convert it into a data frame and split off a validation set that is only used for evaluation purposes.
\item[(2)]
Analyze the data to confirm that the classification problem is non-trivial (i.e. it does not depend on one or two variables only) and to detect variables (not) useful for classification.
\item[(3)]
Choose models to train, estimate the optimal parameters of these models, train them and compare their performance.
\item[(4)]
Choose the best performing model or ensemble several models into a new one to obtain a final model.
\item[(5)]
Evaluate the final model.
\end{itemize}


# Methods
<!-- A methods/analysis section that explains the process and techniques used, including data cleaning, data exploration and visualization, any insights gained, and your modeling approach. At least two different models or algorithms must be used, with at least one being more advanced than linear or logistic regression for prediction problems. -->

## Data wrangling
The raw data was downloaded from
\begin{center}
https://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.data.
\end{center}
It can also be downloaded from the author's Github account following the link
\begin{center}
https://github.com/cgoldner/radar-ionosphere/tree/main/data.
\end{center}

Since the raw data is already in an easy-to-handle format, a minimum of data wrangling is required. More precisely, the column names are set to $x_1, \dots, x_{34}, y$, where $x_1,\dots, x_{34}$ are the measurement results such that $x_i, x_{i+1}$ correspond to the real and imaginary part of a complex number associated to one pulse if $i$ is odd. We may also refer to these variables as \textit{predictor variables}. They are stored as numeric values. The variable $y$, which should be predicted, represents if a measurement is "good" `g` or "bad" `b`. The variable $y$ is stored as factor with levels `g` and `b`. It is important to note that there are no missing values.

Before starting with data exploration and model building, a `validation` set is split off the dataset. It is a hold out set to evaluate the final model's performance. The `validation` set consists of $20\%$ of the overall data. The overall data consists of `r nrow(validation) + nrow(training)` observations and the `validation` set consists of `r nrow(validation)` observations.

```{r, eval=FALSE}
set.seed(1, sample.kind="Rounding") # set seed to make splitting reproducible
val_index <- createDataPartition(ionosphere$y, times = 1, p = 0.2, list = FALSE)
validation <- ionosphere %>% slice(val_index)
training <- ionosphere %>% slice(-val_index)
```

```{r create_val_index, echo=FALSE, warning=FALSE, message =FALSE}
set.seed(1, sample.kind="Rounding") # set seed to make splitting reproducible
val_index <- createDataPartition(ionosphere$y, times = 1, p = 0.2, list = FALSE)
```


## Data exploration
The `training` set is used to explore the data. The following plot clearly shows that real and imaginary parts are differently distributed for each pulse. Interestingly, the first pulse with real part `x_1` and imaginary part `x_2` shows very little variability. More precisely, `x_2` is always equal to zero and thus has no predictive power. Therefore the variable `x_2` can be excluded when building a model.

```{r boxplot_training, echo=FALSE, out.width = "100%", fig.align='center', fig.pos='H'}
boxplot.matrix(as.matrix(training[, which(names(training) != "y")]))
```

To explore how `x_1` can be used to make a prediction, its effect on the measurement type (`g` or `b`) is explored in the following plot.

```{r hist_and_proportion_plot, echo=FALSE, out.width = "90%", fig.align='center', fig.pos='H'}
# look at first columns impact on measurement type (good or bad)
hist_x_1 <- training %>% mutate(y = ifelse(y == "g", "good", "bad")) %>%
  ggplot(aes(x = x_1, fill = y)) +
  geom_histogram(aes(fill = y), binwidth = 0.05) +
  theme(legend.position="none") +
  labs(title = "Effect of x_1 column on measurement type") +
  theme(panel.grid.minor.x=element_blank(),
        panel.grid.major.x=element_blank())

# compare proportion of measurement types
proportion_plot <- training %>% mutate(y = ifelse(y == "g", "good", "bad")) %>%
  group_by(y) %>%
  summarize(n = n()) %>%
  mutate(proportion = n/sum(n)) %>% ungroup() %>%
  mutate(group_var = "Measurement") %>%
  ggplot(aes(group_var, y = proportion, fill = y)) +
  theme_minimal() +
  geom_bar(position = "fill",stat = "identity") +
  theme(legend.position="bottom") +
  labs(fill = "Measurement type",
       x = "", 
       y = "Proportion", 
       title = "Proportion of good and bad measurements") +
  guides(y = "none") +
  geom_hline(yintercept = 0.6428571, size=2) +
  scale_y_continuous(breaks = seq(0, 1, by = 0.1)) +
  coord_flip()

grid.arrange(hist_x_1, proportion_plot, ncol = 1, heights = c(1, 0.5))
```

Notice that `x_1` takes only the values $0$ and $1$, and there are very few $0$ values for `x_1`. Observe that if `x_1` is $0$, then the measurement is a "bad" one. Thus `x_1` can be used to predict "bad" measurements.
Moreover, notice that the plot above shows that the proportion of "good" measurements is around $0.65$. Hence the baseline for the accuracy of our model is $0.65$.

With the following plot, we try to isolate more predictor variables that can be used for classification --- we also need to make sure that our classification problem is non-trivial, i.e. that there are not one or two variables which determine the whole classification.

```{r comparison_plot, echo=FALSE, out.width = "100%", fig.align='center', fig.pos='H'}
# compare all predictor variables
comparison_plot <- training %>%  mutate(y = ifelse(y == "g", "good", "bad")) %>%
  gather(pred_var, measure, -y) %>%
  mutate(pred_var_number = parse_number(pred_var)) %>%
  ggplot(aes(x = factor(pred_var_number), y = measure, fill = y)) +
  geom_boxplot() +
  labs(fill = "Measurement type",
       x = "Predictor variable", 
       y = "Measured value", 
       title = "Comparison of predictor variables") +
  theme(panel.grid.minor.y=element_blank(),
        panel.grid.major.y=element_blank())
comparison_plot
```

Most interquartile ranges overlap and almost all predictor variables show a large variability. Hence the classification problem is non-trivial. However, let us take a closer look to see if the remaining predictor variables (`x_1` and `x_2` are excluded) are correlated in a usable way.

```{r correlation_plot, echo=FALSE, out.width = "100%", fig.align='center', fig.pos='H'}
# inspect correlations between predictor variables
# for that remove x_1 and x_2
corrplot(cor(as.matrix(training[3:34])),
         type = "full", order = "hclust", 
         tl.col = "black", tl.srt = 45)
```

Notice that the correlation matrix above is decomposed into $4$ squares, where the upper left and the lower right square show mostly positive correlations. The upper left square belongs to real parts of pulses and the lower right square belongs to imaginary parts of pulses. Thus, if we want to reduce the number of predictor variables, distinguishing between real and imaginary parts could be useful.

```{r, message = FALSE, warning = FALSE}
# distinguish between real and imaginary parts
training_real <- training[seq(1, 33, 2)]
training_imaginary <- training[seq(2, 34, 2)]
```

To see, if the number of predictor variables could be reduced, three \textit{principal component analyses} (PCAs) are performed: One including all predictor variables, one including only real parts of pulses (`x_i` with $i$ odd) and one including only imaginary parts of pulses (`x_i` with $i$ even).

```{r pca_plot1, echo=FALSE, out.width = "90%", fig.align='center', fig.pos='H'}
# perform PCA to see if number of predictor variables can be reduced
pca_real <- prcomp(scale(training_real))
pca_imaginary <- prcomp(scale(training_imaginary[-1])) # remove x_2 (because sd = 0)
pca <- prcomp(scale(training[1:34][-2])) # remove x_2 (because sd = 0)

df_pc12 <- data.frame(pca$x[, 1:2], outcome = training$y) %>%
  mutate(pca = "all")
df_pc12_real <- data.frame(pca_real$x[, 1:2], outcome = training$y) %>%
  mutate(pca = "real")
df_pc12_imaginary <- data.frame(pca_imaginary$x[, 1:2], outcome = training$y) %>%
  mutate(pca = "imaginary")

# first two principal components do a bad job at distinguishing between good and bad measurements
pc12_plot <- df_pc12 %>% rbind(df_pc12_real) %>%
  rbind(df_pc12_imaginary) %>%
  mutate(pca = factor(pca, levels = c("all", "real", "imaginary"))) %>%
  mutate(outcome = ifelse(outcome == "g", "good", "bad")) %>%
  ggplot(aes(PC1,PC2, fill = outcome)) +
  geom_point(cex=2, pch=21, alpha = 0.7) +
  coord_fixed(ratio = 1) +
  facet_wrap( ~ pca) +
  labs(fill = "Measurement type")
pc12_plot
```

Observe from the plot above, that the first two principal components do a bad job at distinguishing between "good" and "bad" measurements. The plots below show that more than half of the predictor variables are required in order to explain at least $95\%$ of all variance. Thus a PCA cannot be used to remove most predictor variables. Hence we decide not to remove any more predictor variables due to their relatively low number.

```{r pca_plot2, echo=FALSE, out.width = "90%", fig.align='center', fig.pos='H'}
# more than half of predictor variables are required in order to explain at least 95% of variance
# hence there are not a few predictor variables that easily predict the outcome
pca_all_plot <- data.frame(cum_prop = summary(pca)$importance[3, ]) %>% 
  rownames_to_column("prcomp") %>%
  mutate(prcomp = parse_number(prcomp)) %>%
  ggplot(aes(x = prcomp, y = cum_prop)) +
  geom_point(size = 2, color = "burlywood4") +
  scale_y_continuous(breaks = seq(0, 1, 0.1)) +
  scale_x_continuous(breaks = seq(1, 34, 1)) +
  labs(x = "Principal component",
       y = "Cum. prop. of explained var.",
       title = "PCA including real and imaginary parts") +
  geom_hline(yintercept = 0.95, color = "firebrick", linetype = "dashed")

pca_real_imaginary_plot <- data.frame(cum_prop = summary(pca_real)$importance[3, ]) %>% 
  rownames_to_column("prcomp") %>%
  mutate(prcomp = parse_number(prcomp)) %>%
  mutate(pca = "real") %>%
  rbind(
    data.frame(cum_prop = summary(pca_imaginary)$importance[3, ]) %>% 
    rownames_to_column("prcomp") %>%
    mutate(prcomp = parse_number(prcomp)) %>%
    mutate(pca = "imaginary")) %>%
  mutate(pca = factor(pca, levels = c("real", "imaginary"))) %>%
  ggplot(aes(x = prcomp, y = cum_prop, color = pca)) +
  geom_point(size = 2) +
  scale_y_continuous(breaks = seq(0, 1, 0.1)) +
  scale_x_continuous(breaks = seq(1, 17, 1)) +
  labs(x = "Principal component",
       y = "Cum. prop. of explained var.",
       title = "Comparing PCA of real and imaginary parts") +
  geom_hline(yintercept = 0.95, color = "firebrick", linetype = "dashed") +
  scale_color_manual(values = c("deepskyblue4", "orangered"))

grid.arrange(pca_all_plot, pca_real_imaginary_plot, ncol = 1)
```

### Summary of data exploration
The predictor variable `x_1` can be used to predict "bad" measurements. The predictor variable `x_2` has no predictive power and can be excluded when building a model. All of the remaining predictor variables are used when building a model.


## Building the final model
To build a final model, different models are compared and the best ones are chosen.

```{r load_caret, eval = FALSE}
# the "caret" package is used to train models
library(caret)
```

In order to compare different models, they need to be trained and evaluated with optimal tuning parameters.
We do this as follows:


(1) Randomly split `training` into `test` and `train`, where the `train` set consists of $20\%$ of the `training` data.

(2) Optimize model parameters on the `train` set via `caret's` build-in bootstrapping procedure.

(3) Compute accuracy of the optimized model on the `test` set.

(4) Repeat steps 1--3 a number of times, say $k$, and take the average accuracy and average the optimization parameters.

(5) Apply steps 1--4 to all different models.


Having found the best models and their parameters, a final model can be build depending on how each of the different models performed:

(6) Ensemble the best models via majority vote into one model.

(7) Determine if the ensemble performs much better than the best performing model (using steps 1--4). If yes, then the final model is the ensemble. If not, then the final model is defined as the best performing model from above.

The different models that are used are:

\textbf{Regression}
To use Regression techniques, the outcomes "good" and "bad" are encoded as $1$ and $0$, respectively, and a cutoff probability of $0.5$ is used.

- Linear model `lm`.

- Logistic regression model `glm`.

- k-nearest neighbors `knn`\footnote{Although kNN can be used for classification directly, it is used here with $1$s and $0$s}.

\textbf{Classification}

- Generative models `lda` and `qda`.

- Random forest `rf`.

- Support vector machine `svm`.

Notice that no unsupervised models are used since all observations are already labeled.

### Comparing different models

We run the different models each $k$ times, record the accuracies and take their averages to obtain the table below. 

```{r train_models, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
train_models <- function(s,
                         final_training = FALSE,
                         models = c("lm", "glm", "lda", "qda", "knn", "rf", "svm"),
                         tune_grid_rf = data.frame(mtry = seq(1, 10, 1)),
                         tune_grid_svm = expand.grid(scale = c(0, .1, 0.01),
                                                     C = c(0.1, 1, 0.10),
                                                     degree = c(1, 2, 1)),
                         tune_grid_knn = data.frame(k = seq(1, 20, 1))) {
  
    # define train and test set
    if(final_training == FALSE){
      # split off test set for evaluation of model performance
      # split is train = 0.8 and test = 0.2
      set.seed(s, sample.kind="Rounding") # set seed to run function several times with different seeds
      # this simulates cross-validation later on
      test_index <- createDataPartition(training$y, times = 1, p = 0.2, list = FALSE)
      test <- training %>% slice(test_index)
      train <- training %>% slice(-test_index)
    }
    
    if(final_training == TRUE){
      test <- validation
      train <- training
    }
    
    # initialize lists and data frames for saving and returning models and outputs
    model_list <- list()
    model_infos <- list()
    variable_importance <- data.frame(variable = paste0("x_",1:34))
    accuracy <- data.frame()
    predictions <- data.frame(matrix(ncol = nrow(test), nrow = 0))
    names(predictions) <- 1:nrow(test)


  
  # prepare train and test sets
  train_x <- train %>% select(-"x_1", -"x_2", -"y")
  
  train_y_factor <- train %>% pull("y")
  train_y_numeric <- train %>% mutate(y = ifelse(y == "g", 1, 0)) %>% pull("y")
  
  test_factor <- test
  test_numeric <- test %>% mutate(y = ifelse(y == "g", 1, 0))
  
  if("lm" %in% models){
    # lm + cutoff
    fit_lm <- train(train_x,
                    train_y_numeric,
                    method = "lm")
    pred_prop <- predict(fit_lm,
                         newdata = test_numeric)
    pred_lm <- ifelse(pred_prop >= 0.5, "g", "b")
    pred_lm[test_numeric$x_1 == 0] <- "b"
    pred_lm <- pred_lm %>% factor(levels = c("g", "b"))
    acc_lm <- mean(pred_lm == test_factor$y)
    # prepare saving lm results
    var_imp_tmp <- varImp(fit_lm$finalModel)
    names(var_imp_tmp) <- "lm"
    var_imp_tmp <- var_imp_tmp %>%
      mutate(variable = rownames(.))
    # save lm results
    variable_importance <- variable_importance %>%
      left_join(var_imp_tmp, by = "variable")
    accuracy <- accuracy %>%
      rbind(data.frame(model_name = "lm", acc = acc_lm))
    predictions <- predictions %>%
      rbind(data.frame(t(data.frame(pred_lm))))
    model_infos[["lm"]] <- list("lm", fit_lm$finalModel, fit_lm$bestTune)
  }
  
  if("glm" %in% models){
    # glm + cutoff
    fit_glm <- train(train_x,
                     train_y_numeric,
                     method = "glm")
    pred_prop <- predict(fit_glm,
                         newdata = test_numeric)
    pred_glm <- ifelse(pred_prop >= 0.5, "g", "b")
    pred_glm[test_numeric$x_1 == 0] <- "b"
    pred_glm <- pred_glm %>% factor(levels = c("g", "b"))
    acc_glm <- mean(pred_glm == test_factor$y)
    # prepare saving glm results
    var_imp_tmp <- varImp(fit_glm$finalModel)
    names(var_imp_tmp) <- "glm"
    var_imp_tmp <- var_imp_tmp %>%
      mutate(variable = rownames(.))
    # save glm results
    variable_importance <- variable_importance %>%
      left_join(var_imp_tmp, by = "variable")
    accuracy <- accuracy %>%
      rbind(data.frame(model_name = "glm", acc = acc_glm))
    predictions <- predictions %>%
      rbind(data.frame(t(data.frame(pred_glm))))
    model_infos[["glm"]] <- list("glm", fit_glm$finalModel, fit_glm$bestTune)
  }
  
  if("knn" %in% models){
    # knn + cutoff
    fit_knn <- train(train_x,
                     train_y_numeric,
                     method = "knn",
                     tuneGrid = tune_grid_knn)
    pred_prop <- predict(fit_knn$finalModel,
                         newdata = test_numeric  %>%
                           select(-"x_1", -"x_2", -"y"))
    pred_knn <- ifelse(pred_prop >= 0.5, "g", "b")
    pred_knn[test_numeric$x_1 == 0] <- "b"
    pred_knn <- pred_knn %>% factor(levels = c("g", "b"))
    acc_knn <- mean(pred_knn == test_factor$y)
    # save knn results
    accuracy <- accuracy %>%
      rbind(data.frame(model_name = "knn", acc = acc_knn))
    predictions <- predictions %>%
      rbind(data.frame(t(data.frame(pred_knn))))
    model_infos[["knn"]] <- list("knn", fit_knn$finalModel, fit_knn$bestTune)
  }

  if("lda" %in% models){
    # lda
    fit_lda <- train(train_x,
                     train_y_factor,
                     method = "lda")
    pred_prop <- predict(fit_lda$finalModel,
                         newdata = test_factor  %>%
                           select(-"x_1", -"x_2", -"y"))
    pred_lda <- pred_prop$class
    pred_lda[test_numeric$x_1 == 0] <- "b"
    pred_lda <- pred_lda %>% factor(levels = c("g", "b"))
    acc_lda <- mean(pred_lda == test_factor$y)
    # save lda results
    accuracy <- accuracy %>%
      rbind(data.frame(model_name = "lda", acc = acc_lda))
    predictions <- predictions %>%
      rbind(data.frame(t(data.frame(pred_lda))))
    model_infos[["lda"]] <- list("lda", fit_lda$finalModel, fit_lda$bestTune)
  }
  
  if("qda" %in% models){
    # qda
    fit_qda <- train(train_x,
                     train_y_factor,
                     method = "qda")
    pred_prop <- predict(fit_qda$finalModel,
                         newdata = test_factor  %>%
                           select(-"x_1", -"x_2", -"y"))
    pred_qda <- pred_prop$class
    pred_qda[test_numeric$x_1 == 0] <- "b"
    pred_qda <- pred_qda %>% factor(levels = c("g", "b"))
    acc_qda <- mean(pred_qda == test_factor$y)
    # save qda results
    accuracy <- accuracy %>%
      rbind(data.frame(model_name = "qda", acc = acc_qda))
    predictions <- predictions %>%
      rbind(data.frame(t(data.frame(pred_qda))))
    model_infos[["qda"]] <- list("qda", fit_qda$finalModel, fit_qda$bestTune)
  }
   
  if("rf" %in% models){   
    # rf
    # do not scale data since input data is already between -1 and 1
    # we assume input data has gone through some kind of scaling already
    # moreover, scaling with scale(train_x) yields worse results
    # do not exclude x_1 since rf incorporates x_1 automatically
    fit_rf <- train(train %>% select(-"x_2", -"y"),
                    train_y_factor,
                    method = "rf",
                    metric = "Accuracy",
                    tuneGrid = tune_grid_rf)
    pred_rf <- predict(fit_rf$finalModel,
                       newdata = test_factor  %>%
                         select(-"x_2", -"y"))
    pred_rf <- pred_rf %>% factor(levels = c("g", "b"))
    acc_rf <- mean(pred_rf == test_factor$y)
    # prepare saving rf results
    var_imp_tmp <- varImp(fit_rf$finalModel)
    names(var_imp_tmp) <- "rf"
    var_imp_tmp <- var_imp_tmp %>%
      mutate(variable = rownames(.))
    # save rf results
    variable_importance <- variable_importance %>%
      left_join(var_imp_tmp, by = "variable")
    accuracy <- accuracy %>%
      rbind(data.frame(model_name = "rf", acc = acc_rf))
    predictions <- predictions %>%
      rbind(data.frame(t(data.frame(pred_rf))))
    model_infos[["rf"]] <- list("rf", fit_rf$finalModel, fit_rf$bestTune)
  }
  
  if("svm" %in% models){
    # svm
    # do not scale data since input data is already between -1 and 1
    # we assume input data has gone through some kind of scaling already
    # moreover, scaling with scale(train_x) yields worse results
    # do not exclude x_1 since SVM incorporates x_1 automatically
    fit_svm <- train(train %>% select(-"x_2", -"y"),
                 train_y_factor,
                 method = "svmPoly",
                 tuneLength = 4, 
                 tuneGrid = tune_grid_svm)
    pred_svm <- predict(fit_svm,
                       newdata = test_numeric  %>%
                         select(-"x_2", -"y"))
    pred_svm <- pred_svm %>% factor(levels = c("g", "b"))
    acc_svm <- mean(pred_svm == test_factor$y)
    # prepare saving svm results
    var_imp_tmp <- varImp(fit_svm)$importance["g"]
    names(var_imp_tmp) <- "svm"
    var_imp_tmp <- var_imp_tmp %>%
      mutate(variable = rownames(.))
    # save svm results
    variable_importance <- variable_importance %>%
      left_join(var_imp_tmp, by = "variable")
    accuracy <- accuracy %>%
      rbind(data.frame(model_name = "svm", acc = acc_svm))
    predictions <- predictions %>%
      rbind(data.frame(t(data.frame(pred_svm))))
    model_infos[["svm"]] <- list("svm", fit_svm, fit_svm$bestTune)
  }  

  # process saved results
  predictions <- predictions %>% 
    unname() %>%
    t() %>%
    data.frame() %>%
    lapply(as.factor) %>%
    data.frame() %>%
    cbind(y = test_factor$y)
  rownames(variable_importance) <- variable_importance$variable
  variable_importance <- variable_importance %>%
    select(-variable) %>% 
    t() %>%
    as.data.frame()
  
  # pack all results into a list and return it
  model_list <- list(model_infos, variable_importance, accuracy, predictions)
  names(model_list) <- c("model_infos", "variable_importance", "accuracy", "predictions")
  
  # the output of the train_models function is "model_list":
  # model_list = list over all trained models:
  # list(list(model_infos), df(variable_importance), df(accuracy), df(predictions))
  # with model_infos = list("model_name", "tuned_model", "tuning_parameters")
  
  model_list
}
```


```{r run_models, warnings = FALSE, echo=FALSE, include=FALSE}
# Run all models on train set to tune their parameters --------------------
#
# run k times with random test sets
# then choose the best models
# model_list_train is a list of length k
# each entry is a the output of one run of the train_models function

k <- 10
model_list_train <- list()

for (s in 1:k) {
  model_list_train[[s]] <- train_models(s, final_training = FALSE)
}
```

```{r acc_table_all_models, echo = FALSE, include = FALSE}
accuracy_train_df <- data.frame(model_name = model_list_train[[1]]$accuracy$model_name)

for (s in 1:k) {
  accuracy_train_df <- accuracy_train_df %>%
    left_join(model_list_train[[s]]$accuracy, by = "model_name")
  accuracy_train_df <- accuracy_train_df %>% rename_at("acc", list(~paste0("run_", s)))
}

table_train_acc <- accuracy_train_df %>%
  mutate(avg = rowMeans(accuracy_train_df[, colnames(accuracy_train_df) != "model_name"])) %>%
  arrange(desc(avg)) %>% 
  rename(model = model_name) %>% 
  mutate(across(where(is.numeric), round, 3))
```

```{r, warning=FALSE, message =FALSE, echo=FALSE}
kable(table_train_acc, caption = "Average accuracies of models over $k = 10$ runs.", booktabs = TRUE) %>%         kable_styling(latex_options=c("scale_down","striped", "HOLD_position"))
```


Notice that `lm`, `glm` and `lda` perform the same, which is not surprising since they classify the outcomes by using a linear boundary. Hence the predictions of `lm`, `glm` and `lda` should be the same. This can be confirmed using the following code.

```{r, eval = FALSE}
# Check if lm, glm, lda yield the same results
for (s in 1:k) {
  print(identical(model_list_train[[s]]$predictions$pred_lm,
                  model_list_train[[s]]$predictions$pred_lda))
}

for (s in 1:k) {
  print(identical(model_list_train[[s]]$predictions$pred_glm,
                  model_list_train[[s]]$predictions$pred_lda))
}
```

Therefore we do not use `lm` and `glm` to build the final model.
Thus the three best performing models are `rf`, `svm` and `lda`.


### Evaluating tuning parameters

```{r mtry, echo=FALSE, message=FALSE, warning=FALSE}
# tuning parameter mtry for rf
rf_mtry <- data.frame()

for (s in 1:k) {
  rf_mtry <- rf_mtry %>% rbind(model_list_train[[s]]$model_infos$rf[[3]])
}

rf_mtry <- rf_mtry %>% summarize(mtry = round(mean(mtry)))
```

To find the best tuning parameter `mtry` for `rf`, the mean of all optimal `mtry` parameters of the $k$ different runs is taken, which is `r rf_mtry$mtry`. 

To tune `svm`, different ranges to find the optimal parameters in the single runs were tested. It can be observed that the performance decreased when allowing `svm` to fit with degrees $> 2$, i.e. these degrees lead to overfitting of `svm`. However, notice that, if tuning parameters are chosen correctly, then, in general, `svm` should not perform worse than linear models since it generalizes linear approaches. After comparing perfomances of degree $1$ and degree $2$, degree $2$ is taken. We observe that optimizing the remaining tuning parameters yields the same results in all runs. Thus these are the parameters we choose when building a final model.

```{r svm_tune, echo=FALSE, message=FALSE, warning=FALSE, results= 'hide'}
# for svm the tuned parameters are almost constant
# notice that if we allow higher degrees to be used for tuning in svm
# then accuracy of svm suffers from overfitting:
# train_acc degree = 3: 0.8946429
# train_acc degree = 2: 0.9107143 -> d=2 is best degree
# train_acc degree = 1: 0.8803571
# in general svm should not perform worse than linear methods
# since it generalizes linear approaches
for (s in 1:k) {
  print(model_list_train[[s]]$model_infos$svm[[3]])
}

svm_tune <- model_list_train[[s]]$model_infos$svm[[3]]
```

### Ensemble model
The three best performing models `rf`, `svm` and `lda` are now ensembled into a single model using majority vote. Then, as before, the ensembled model is run $k$ times and the resulting accuracies are averaged.

```{r ensemble_model, echo=FALSE, warning=FALSE, message = FALSE}
# ensemble rf, svm, lda
train_ensemble <- function(s, final_training = FALSE) {
  ensemble_list <- train_models(s, final_training,
                                models = c("rf", "svm", "lda"),
                                tune_grid_rf = rf_mtry,
                                tune_grid_svm = svm_tune)
  pred_ensemble <- ensemble_list$predictions %>%
    mutate(votes_good = ifelse(pred_lda == "g", 1, 0)) %>%
    mutate(votes_good = ifelse(pred_rf == "g", votes_good + 1, votes_good)) %>% 
    mutate(votes_good = ifelse(pred_svm == "g", votes_good + 1, votes_good)) %>%
    mutate(pred_ensemble = ifelse(votes_good >= 2, "g", "b")) %>%
    mutate(pred_ensemble = factor(pred_ensemble, levels = c("g", "b"))) %>% 
    select(-"votes_good")
}

k <- 10
pred_ensemble_list <- list()

for (s in 1:k) {
  pred_ensemble_list[[s]] <- train_ensemble(s, final_training = FALSE)
}

accuracy_ensemble_df <- data.frame()
for (s in 1:k) {
  accuracy_ensemble_df <- rbind(accuracy_ensemble_df, 
                                mean(pred_ensemble_list[[s]]$pred_ensemble == pred_ensemble_list[[s]]$y))
}
names(accuracy_ensemble_df) <- "acc_ensemble_per_run"
acc_ensemble <- accuracy_ensemble_df %>%
  summarize(round(mean(acc_ensemble_per_run, digits = 3)))
```

\begin{center}
```{r acc_table_ensemble, echo = FALSE}
acc_comparison <- table_train_acc %>%
    filter(model %in% c("rf", "svm", "lda")) %>%
    select(model, avg) %>%
    t() %>%
    cbind(c("ensemble", acc_ensemble[1,1])) %>%
    data.frame() %>%
    unname()
names(acc_comparison) <- acc_comparison["model",]
acc_comparison[2, ] %>%
  mutate(ensemble = round(as.numeric(ensemble), digits = 7)) %>%
  kable(caption = "Accuracies of different models.", booktabs = TRUE) %>% 
  kable_styling(latex_options=c("HOLD_position"))
```
\end{center}

```{r confusion_matrices_ensemble_vs_rf, warning=FALSE}
# Compare types of errors of rf and ensemble ------------------------------
# ensemble misclassifies "bad" as "good" more often than rf does
# i.e. lower specificity

model_ensemble <- train_ensemble(s = 1, final_training = FALSE)

# confusion matrix rf
confusionMatrix(model_ensemble$pred_rf, model_ensemble$y)$table

# confusion matrix ensemble
confusionMatrix(model_ensemble$pred_ensemble, model_ensemble$y)$table
```

The results above show that the ensemble does not perform much better than `rf`. Thus the simpler model, namely `rf`, is chosen as final model.

```{r final_model, echo=FALSE, message=FALSE, warning=FALSE}
# Final model
final_model <- train_models(s = 1, final_training = TRUE,
             models = c("rf"),
             tune_grid_rf = rf_mtry)
```


### Summary of model building
Random forest `rf` turned out to be the best performing model. Its performance is similar to that of an ensemble via majority vote of `rf`, `svm` and `lda`. Since `rf` is the simpler model, it is chosen as the final model with optimized model parameter `mtry`.

# Results
<!-- a results section that presents the modeling results and discusses the model performance; -->

## Accuracy

```{r acc_final_model, echo=FALSE, warning=FALSE, message=FALSE}
# Evaluate overall performance of final model -----------------------------
acc_final_model <- confusionMatrix(final_model$predictions$pred_rf, final_model$predictions$y)$overall["Accuracy"]
F_meas_final_model <- F_meas(data = factor(final_model$predictions$pred_rf, levels = c("g", "b")), reference = final_model$predictions$y)
```

After defining `rf` to be our final model (with optimized tuning parameter `mtry` given by `r rf_mtry$mtry`), its accuracy is measured on the final hold set `validation` as `r round(acc_final_model, digits = 3)`, and, additionally, its $F_1$ value is `r round(F_meas_final_model, digits = 3)`. The following confusion matrix shows that the final model still suffers from misclassifying "bad" as "good", i.e. it has lower specificity than sensitivity if the positive class is "good"

```{r confusion_matrix_final_model, warning=FALSE, message=FALSE}
# confusion matrix for final model on validation set
confusionMatrix(final_model$predictions$pred_rf, final_model$predictions$y)
```

## Evaluation of errors
To better understand the errors made by our final model, the variable importance of each variable is evaluated and compared with the value of that variable in each missclassified measurement. More precisely, the upper part of the plot below shows the variable importance of each variable in the final model, where the overall average importance, and the averages of real and imaginary parts of pulses are indicated. The lower part of the plot below shows the "unusualness" of each measurement of each variable using $z$--scores. Notice that, the claim that "misclassified observations are those whose real and imaginary parts of the pulses are very unusual, or at least unusual in the most important variables" is only partly true. The false negative classification "Misclassification $4$" seems to be "unusual" in a lot of variables associated to imaginary parts: the $z$--scores of `x_18`, `x_22`, `x_24`, `x_26`, `x_28`, `x_32` and `x_34` are around $2$. Since there are not enough false negatives, it cannot be deduced that an accumulation of unusual values in the imaginary parts leads to misclassification. Moreover, in case of false positives, there are less "unusual" values. Observe that no single predictor variable of "Misclassification 2" has an unusual value (i.e. $z$--score $>2$).

In total, we are not able to trace the misclassifications back to unusual values of important variables. 

```{r var_imp_plot, echo=FALSE, out.width = "100%", fig.align='center', fig.pos='H', fig.height = 15}
# Evaluate variable importance in final model -----------------------------
evaluation_variable_importance <- final_model$variable_importance %>%
  select(-"x_2") %>%
  gather(variable, importance) %>%
  mutate(variable = parse_number(variable)) %>%
  mutate(real = ifelse(variable %% 2 == 0 , "imaginary", "real")) %>% # modulo division by 2
  mutate(real = factor(real, levels = c("real", "imaginary")))
avg_importance_real_imaginary <- evaluation_variable_importance %>% 
  group_by(real) %>% 
  summarize(avg = mean(importance))
avg_importance <- evaluation_variable_importance %>%
  summarize(avg = mean(importance))

# observe that few variables are very important
variable_importance_plot <- evaluation_variable_importance %>% 
  ggplot(aes(x = factor(variable), y = importance, fill = real)) +
  theme(legend.position="top") + 
  geom_col() +
  scale_fill_manual(values = c("deepskyblue4", "orangered")) +
  geom_hline(yintercept = filter(avg_importance_real_imaginary, real == "real")$avg,
             color = "deepskyblue4") +
  geom_hline(yintercept = filter(avg_importance_real_imaginary, real == "imaginary")$avg,
             color = "orangered") +
  geom_hline(yintercept = avg_importance$avg,
             color = "black") +
  labs(x = "Predictor variable",
       y = "Importance",
       title = "Variable importance and error type", 
       fill = "")

# Evaluate errors of final model ------------------------------------------
unlikely_values_and_misclassification <- scale(ionosphere %>% select(-"y", -"x_2")) %>%
  data.frame() %>%
  slice(val_index) %>%
  cbind(final_model$predictions) %>%
  filter(pred_rf != y) %>% # filter misclassified
  mutate(error_type = factor(ifelse(y == "b", "FP", "FN"))) %>% 
  mutate(observation_number = 1:length(error_type)) %>%
  select(-"pred_rf", -"y") %>% 
  gather(variable, measure, -"error_type", -"observation_number") %>% 
  mutate(variable = parse_number(variable)) %>%
  mutate(observation_number = paste0("Misclassification ", observation_number)) %>% 
  ggplot(aes(x = factor(variable), y = measure, color=error_type)) +
  theme(panel.grid.minor.y=element_blank(),
        panel.grid.major.y=element_blank(),
        legend.position="bottom") +
  geom_point() +
  scale_y_continuous(limits = c(-3, 3)) +
  geom_hline(yintercept = 2, color = "firebrick", linetype = "dashed") +
  geom_hline(yintercept = -2, color = "firebrick", linetype = "dashed") +
  facet_wrap(observation_number ~ ., ncol = 1) +
  labs(fill = "Error type",
       x = "Predictor variable", 
       y = "z-score of measured value") +
  scale_color_manual(values = c("darkorchid2", "navy"))

grid.arrange(variable_importance_plot, unlikely_values_and_misclassification, ncol = 1, heights = c(4, 10))
```




# Conclusion
<!-- a conclusion section that gives a brief summary of the report, its potential impact, its limitations, and future work. -->

## Summary
Random forest turned out to be the best model to classify the given measurements into "good" and "bad" ones. The accuracy achieved was `r round(acc_final_model, digits = 3)` which is about `r round(acc_final_model - 0.65, digits = 2)` higher than the baseline.

## Limitations
The total number of observations used to build and to test the final model was very small. Thus validating the model produces very few errors (less than 10), which in turn makes it hard to identify common properties of these errors.

## Future improvements
The ensemble of `rf`, `svm` and `lda` we build showed a similar performance to the final model `rf` in the testing phase. Thus by increasing the performance of the weakest model among `rf`, `svm`, `lda`, which is `lda`, it could be possible to make the ensemble perform better than `rf`. For that, the `lda` model could be adapted to take prevalence into account. This would increase the performance of `lda` since "good" and "bad" measurements do not appear with the same frequency .

